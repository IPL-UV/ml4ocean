{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo I - Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Important** - Paths and Directories\n",
    "\n",
    "This is annoying but it needs to be defined otherwise things get confusing. We need a few important paths to be pre-defined:\n",
    "\n",
    "| Name | Variable | Purpose |\n",
    "| ---| --- | --- |\n",
    "| Project | `PROJECT_PATH` | top level directory for the project (assuming megatron) |\n",
    "| Code |  `CODE_PATH` | folder of any dedicated functions that we use |\n",
    "| Raw Data | `RAW_PATH` | where the raw data is. Ideally, we **never** touch this ever except to read. |\n",
    "| Processed Data | `DATA_PATH` | where the processed data is stored |\n",
    "| Interim Data | `INTERIM_PATH` | where we save the training, validation and testing data |\n",
    "| Saved Models | `MODEL_PATH` | where we save any trained models |\n",
    "| Results Data | `RESULTS_PATH` | where we save any data results or outputs from ML models |\n",
    "| Figures | `FIG_PATH` | where we store any plotted figures during any part of our ML pipeline|\n",
    "\n",
    "This cell checks to see if all of the paths exist. If there is a path missing, it probably means you're not in megatron. If that's the case...well, we'll cross that bridge when we get there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "# define the top level directory\n",
    "PROJECT_PATH = pathlib.Path(\"/media/disk/erc/papers/2019_ML_OCN/\")\n",
    "CODE_PATH = PROJECT_PATH.joinpath(\"ml4ocean\", \"src\")\n",
    "\n",
    "# check if path exists and is a directory\n",
    "assert PROJECT_PATH.exists() & PROJECT_PATH.is_dir()\n",
    "assert CODE_PATH.exists() & CODE_PATH.is_dir()\n",
    "\n",
    "# add code and project paths to PYTHONPATH (to call functions)\n",
    "sys.path.append(str(PROJECT_PATH))\n",
    "sys.path.append(str(CODE_PATH))\n",
    "\n",
    "# specific paths\n",
    "FIG_PATH = PROJECT_PATH.joinpath(\"ml4ocean/reports/figures/global/\")\n",
    "RAW_PATH = PROJECT_PATH.joinpath(\"data/global/raw/\")\n",
    "DATA_PATH = PROJECT_PATH.joinpath(\"data/global/processed/\")\n",
    "INTERIM_PATH = PROJECT_PATH.joinpath(\"data/global/interim/\")\n",
    "MODEL_PATH = PROJECT_PATH.joinpath(\"models/global/\")\n",
    "RESULTS_PATH = PROJECT_PATH.joinpath(\"data/global/results/\")\n",
    "\n",
    "# check if path exists and is a directory\n",
    "assert FIG_PATH.exists() & FIG_PATH.is_dir()\n",
    "assert RAW_PATH.exists() & RAW_PATH.is_dir()\n",
    "assert DATA_PATH.exists() & DATA_PATH.is_dir()\n",
    "assert INTERIM_PATH.exists() & INTERIM_PATH.is_dir()\n",
    "assert MODEL_PATH.exists() & MODEL_PATH.is_dir()\n",
    "assert RESULTS_PATH.exists() & RESULTS_PATH.is_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard packages\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Global Data\n",
    "\n",
    "In this section, I will load the metadata and the actual data. The steps involved are:\n",
    "\n",
    "1. Define the filepath (check for existence)\n",
    "2. Open meta data and real data\n",
    "3. Check that the samples correspond to each other.\n",
    "4. Check if # of features are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wmo</th>\n",
       "      <th>n_cycle</th>\n",
       "      <th>N</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>juld</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2902086</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>88.695687</td>\n",
       "      <td>12.163850</td>\n",
       "      <td>23009.165972</td>\n",
       "      <td>2012-12-30 03:58:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2902086</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>88.603349</td>\n",
       "      <td>12.412847</td>\n",
       "      <td>23018.142361</td>\n",
       "      <td>2013-01-08 03:24:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2902086</td>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>86.203895</td>\n",
       "      <td>13.791507</td>\n",
       "      <td>23432.149305</td>\n",
       "      <td>2014-02-26 03:34:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2902086</td>\n",
       "      <td>101</td>\n",
       "      <td>65</td>\n",
       "      <td>86.311614</td>\n",
       "      <td>13.750043</td>\n",
       "      <td>23437.143750</td>\n",
       "      <td>2014-03-03 03:26:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2902086</td>\n",
       "      <td>102</td>\n",
       "      <td>66</td>\n",
       "      <td>86.397120</td>\n",
       "      <td>13.758830</td>\n",
       "      <td>23442.147222</td>\n",
       "      <td>2014-03-08 03:31:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       wmo  n_cycle   N        lon        lat          juld  \\\n",
       "0  2902086        1   1  88.695687  12.163850  23009.165972   \n",
       "1  2902086       10  10  88.603349  12.412847  23018.142361   \n",
       "2  2902086      100  64  86.203895  13.791507  23432.149305   \n",
       "3  2902086      101  65  86.311614  13.750043  23437.143750   \n",
       "4  2902086      102  66  86.397120  13.758830  23442.147222   \n",
       "\n",
       "                  date  \n",
       "0  2012-12-30 03:58:59  \n",
       "1  2013-01-08 03:24:59  \n",
       "2  2014-02-26 03:34:59  \n",
       "3  2014-03-03 03:26:59  \n",
       "4  2014-03-08 03:31:59  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name of file\n",
    "meta_name = \"METADATA_20200310.csv\"\n",
    "\n",
    "# get full path\n",
    "meta_file = DATA_PATH.joinpath(meta_name)\n",
    "\n",
    "# assert meta file exists\n",
    "error_msg = f\"File '{meta_file.name}' doesn't exist. Check name or directory.\"\n",
    "assert meta_file.exists(), error_msg\n",
    "\n",
    "# assert meta file is a file\n",
    "error_msg = f\"File '{meta_file.name}' isn't a file. Check name or directory.\"\n",
    "assert meta_file.is_file(), error_msg\n",
    "\n",
    "# open meta data\n",
    "meta_df = pd.read_csv(f\"{meta_file}\",sep=',')\n",
    "\n",
    "#ANA: I got error \"AttributeError: 'DataFrame' object has no attribute 'to_markdown'\"\"\n",
    "#meta_df.head().to_markdown()\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25413, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>wmo</th>\n",
       "      <th>n_cycle</th>\n",
       "      <th>sla</th>\n",
       "      <th>PAR</th>\n",
       "      <th>RHO_WN_412</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2902086</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.704400</td>\n",
       "      <td>42.6541</td>\n",
       "      <td>0.025462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2902086</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.038200</td>\n",
       "      <td>42.6541</td>\n",
       "      <td>0.025462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2902086</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.460399</td>\n",
       "      <td>44.2927</td>\n",
       "      <td>0.024094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2902086</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.840400</td>\n",
       "      <td>42.7664</td>\n",
       "      <td>0.024917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2902086</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.394000</td>\n",
       "      <td>42.7664</td>\n",
       "      <td>0.024917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2902086</td>\n",
       "      <td>6</td>\n",
       "      <td>-2.049000</td>\n",
       "      <td>42.7468</td>\n",
       "      <td>0.025830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2902086</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.772300</td>\n",
       "      <td>42.7468</td>\n",
       "      <td>0.025830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2902086</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.429900</td>\n",
       "      <td>42.6694</td>\n",
       "      <td>0.025811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2902086</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.261000</td>\n",
       "      <td>44.5087</td>\n",
       "      <td>0.020570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2902086</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.901500</td>\n",
       "      <td>44.5505</td>\n",
       "      <td>0.020603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    N      wmo  n_cycle       sla      PAR  RHO_WN_412\n",
       "0   1  2902086        1 -4.704400  42.6541    0.025462\n",
       "1   2  2902086        2 -4.038200  42.6541    0.025462\n",
       "2   3  2902086        3 -3.460399  44.2927    0.024094\n",
       "3   4  2902086        4 -2.840400  42.7664    0.024917\n",
       "4   5  2902086        5 -2.394000  42.7664    0.024917\n",
       "5   6  2902086        6 -2.049000  42.7468    0.025830\n",
       "6   7  2902086        7 -1.772300  42.7468    0.025830\n",
       "7   8  2902086        8 -1.429900  42.6694    0.025811\n",
       "8   9  2902086        9 -1.261000  44.5087    0.020570\n",
       "9  10  2902086       10 -0.901500  44.5505    0.020603"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name of file\n",
    "data_name = \"SOCA_GLOBAL2_20200310.csv\"\n",
    "\n",
    "# get full path\n",
    "data_file = DATA_PATH.joinpath(data_name)\n",
    "\n",
    "# assert exists\n",
    "error_msg = f\"File '{data_file.name}' doesn't exist. Check name or directory.\"\n",
    "assert data_file.exists(), error_msg\n",
    "\n",
    "# assert meta file is a file\n",
    "error_msg = f\"File '{data_file.name}' isn't a file. Check name or directory.\"\n",
    "assert data_file.is_file(), error_msg\n",
    "\n",
    "# load data\n",
    "data_df = pd.read_csv(f\"{data_file}\")\n",
    "\n",
    "## Same markdown error here\n",
    "#data_df.head().iloc[:, :6].to_markdown()\n",
    "data_df.iloc[0:10, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25413, 48)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Checks\n",
    "\n",
    "I do a number of checks to make sure that our data follows a standard and that I am reproducing the same results.\n",
    "\n",
    "* Number of samples are equal for both\n",
    "* 7 meta features \n",
    "* 48 data features (26 data + 19 levels + 3 meta)\n",
    "* check features in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same number of samples\n",
    "error_msg = f\"Mismatch between meta and data: {data_df.shape[0]} =/= {meta_df.shape[0]}\"\n",
    "assert data_df.shape[0] == meta_df.shape[0], error_msg\n",
    "\n",
    "# check number of samples\n",
    "n_samples = 25413\n",
    "error_msg = f\"Incorrect number of samples: {data_df.shape[0]} =/= {n_samples}\"\n",
    "assert data_df.shape[0] == n_samples, error_msg\n",
    "\n",
    "# check meta feature names\n",
    "meta_features = ['wmo', 'n_cycle', 'N', 'lon', 'lat', 'juld', 'date']\n",
    "error_msg = f\"Missing features in meta data.\"\n",
    "assert meta_df.columns.tolist() == meta_features, error_msg\n",
    "\n",
    "# check data feature names\n",
    "input_meta_features = ['N', 'wmo', 'n_cycle']\n",
    "input_features = ['sla', 'PAR', 'RHO_WN_412', 'RHO_WN_443',\n",
    "       'RHO_WN_490', 'RHO_WN_555', 'RHO_WN_670', 'doy_sin', 'doy_cos',\n",
    "       'x_cart', 'y_cart', 'z_cart', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6',\n",
    "       'PC7', 'PC1.1', 'PC2.1', 'PC3.1', 'PC1.2', 'PC2.2', 'PC3.2', 'PC4.1']\n",
    "output_features = ['bbp', 'bbp.1', 'bbp.2', 'bbp.3', 'bbp.4', 'bbp.5', 'bbp.6', 'bbp.7',\n",
    "       'bbp.8', 'bbp.9', 'bbp.10', 'bbp.11', 'bbp.12', 'bbp.13', 'bbp.14',\n",
    "       'bbp.15', 'bbp.16', 'bbp.17', 'bbp.18']\n",
    "features = input_meta_features + input_features + output_features\n",
    "error_msg = f\"Missing features in input data.\"\n",
    "assert data_df.columns.tolist() == features, error_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Convert metadata to indices (**Important**)\n",
    "\n",
    "To make our life easier, we're going to eliminate the need to keep track of meta data all of the time. So I'm going to merge the datasets together to form one dataframe. Then I will set the index to be the metadata values. The remaining parts will be columns which will be features. \n",
    "\n",
    "So in the end, we will have a dataframe where:\n",
    "\n",
    "* the **indices** is the metadata (e.g. wmo, n_cycle) \n",
    "* the **columns** are the actual features (e.g. sla, pca components, bbp, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge meta and data\n",
    "full_df = pd.merge(meta_df, data_df)\n",
    "\n",
    "# convert meta information to indices\n",
    "full_df = full_df.set_index(meta_features)\n",
    "\n",
    "# checks - check indices match metadata\n",
    "meta_features = ['wmo', 'n_cycle', 'N', 'lon', 'lat', 'juld', 'date']\n",
    "error_msg = f\"Missing features in input data.\"\n",
    "assert full_df.index.names == meta_features, error_msg\n",
    "\n",
    "# checks - check column names match feature names\n",
    "input_features = ['sla', 'PAR', 'RHO_WN_412', 'RHO_WN_443',\n",
    "       'RHO_WN_490', 'RHO_WN_555', 'RHO_WN_670', 'doy_sin', 'doy_cos',\n",
    "       'x_cart', 'y_cart', 'z_cart', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6',\n",
    "       'PC7', 'PC1.1', 'PC2.1', 'PC3.1', 'PC1.2', 'PC2.2', 'PC3.2', 'PC4.1']\n",
    "output_features = ['bbp', 'bbp.1', 'bbp.2', 'bbp.3', 'bbp.4', 'bbp.5', 'bbp.6', 'bbp.7',\n",
    "       'bbp.8', 'bbp.9', 'bbp.10', 'bbp.11', 'bbp.12', 'bbp.13', 'bbp.14',\n",
    "       'bbp.15', 'bbp.16', 'bbp.17', 'bbp.18']\n",
    "features = input_features + output_features\n",
    "error_msg = f\"Missing features in input data.\"\n",
    "assert full_df.columns.tolist() == features, error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sla', 'PAR', 'RHO_WN_412', 'RHO_WN_443', 'RHO_WN_490', 'RHO_WN_555',\n",
       "       'RHO_WN_670', 'doy_sin', 'doy_cos', 'x_cart', 'y_cart', 'z_cart', 'PC1',\n",
       "       'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC1.1', 'PC2.1', 'PC3.1',\n",
       "       'PC1.2', 'PC2.2', 'PC3.2', 'PC4.1', 'bbp', 'bbp.1', 'bbp.2', 'bbp.3',\n",
       "       'bbp.4', 'bbp.5', 'bbp.6', 'bbp.7', 'bbp.8', 'bbp.9', 'bbp.10',\n",
       "       'bbp.11', 'bbp.12', 'bbp.13', 'bbp.14', 'bbp.15', 'bbp.16', 'bbp.17',\n",
       "       'bbp.18'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Features: (25413, 45)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sla',\n",
       " 'PAR',\n",
       " 'RHO_WN_412',\n",
       " 'RHO_WN_443',\n",
       " 'RHO_WN_490',\n",
       " 'RHO_WN_555',\n",
       " 'RHO_WN_670',\n",
       " 'doy_sin',\n",
       " 'doy_cos',\n",
       " 'x_cart']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Dataframe Features:', full_df.shape)\n",
    "full_df.columns.tolist()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Indices (meta vars): 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FrozenList(['wmo', 'n_cycle', 'N', 'lon', 'lat', 'juld', 'date'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Dataframe Indices (meta vars):', len(full_df.index.names))\n",
    "full_df.index.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Training and Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Independent Set I (SOCA2016)\n",
    "\n",
    "This independent set has a set number of independent floats which are not counted in the training or validation phase. These floats were in a paper (Sauzede et. al., 2016) and used during the testing phase to showcase how well the models did.\n",
    "\n",
    "* 6901472\n",
    "* 6901493\n",
    "* 6901523\n",
    "* 6901496\n",
    "\n",
    "So we need to take these away from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soca2016 independent floats\n",
    "soca2016_floats = [\"6901472\", \"6901493\", \"6901523\", \"6901496\"]\n",
    "\n",
    "# subset soca2016 floats\n",
    "soca2016_df = full_df[full_df.index.isin(soca2016_floats, level='wmo')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378, 45)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soca2016_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of samples (meta, inputs)\n",
    "n_samples = 378\n",
    "error_msg = f\"Incorrect number of samples for soca2016 floats: {soca2016_df.shape[0]} =/= {n_samples}\"\n",
    "assert soca2016_df.shape[0] == n_samples, error_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Ana: Why 378 rows if there are 4 floats? I guess they are not the same length. Just to have it clear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Indpendent Set II (ISPRS2020)\n",
    "\n",
    "This independent set was a set of floats taken from the ISPRS paper (Sauzede et. al., 2020 (pending...)). These floats were used as the independent testing set to showcase the performance of the ML methods.\n",
    "\n",
    "* 6901486 (North Atlantic?)\n",
    "* 3902121 (Subtropical Gyre?)\n",
    "\n",
    "So we need to take these away from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isprs2020 independent floats\n",
    "isprs2020_floats = [\"6901486\", \"3902121\"]\n",
    "\n",
    "# isprs2020 independent floats\n",
    "isprs2020_df = full_df[full_df.index.isin(isprs2020_floats, level='wmo')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(331, 45)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isprs2020_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of samples (meta, inputs)\n",
    "n_samples = 331\n",
    "error_msg = f\"Incorrect number of samples for isprs2016 floats: {isprs2020_df.shape[0]} =/= {n_samples}\"\n",
    "assert isprs2020_df.shape[0] == n_samples, error_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - ML Data\n",
    "\n",
    "Now we want to subset the input data to be used for the ML models. Basically, we can subset all datasets that **are not** in the independent floats. In addition, we want all of the variables in the input features that we provided earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset non-independent flows\n",
    "ml_df = full_df[~full_df.index.isin(isprs2020_floats + soca2016_floats, level='wmo')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24704, 45)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of samples (meta, inputs)\n",
    "n_samples = 24704\n",
    "error_msg = f\"Incorrect number of samples for non-independent floats: {ml_df.shape[0]} =/= {n_samples}\"\n",
    "assert ml_df.shape[0] == n_samples, error_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Inputs, Outputs\n",
    "\n",
    "Lastly, we need to split the data into training, validation (and possibly testing). Recall that all the inputs are already written above and the outputs as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = ml_df[input_features]\n",
    "output_df = ml_df[output_features]\n",
    "\n",
    "# checks - Input Features\n",
    "n_input_features = 26\n",
    "error_msg = f\"Incorrect number of features for input df: {input_df.shape[1]} =/= {n_input_features}\"\n",
    "assert input_df.shape[1] == n_input_features, error_msg\n",
    "\n",
    "# checks - Output Features\n",
    "n_output_features = 19\n",
    "error_msg = f\"Incorrect number of features for output df: {output_df.shape[1]} =/= {n_output_features}\"\n",
    "assert output_df.shape[1] == n_output_features, error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24704, 26), (24704, 19))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.shape, output_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Final Dataset (saving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Print out data dimensions (w. metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data: (24704, 26)\n",
      "Output Data: (24704, 19)\n",
      "SOCA2016 Independent Data: (378, 26) (378, 19)\n",
      "ISPRS2016 Independent Data: (331, 26) (331, 19)\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Data:\", input_df.shape)\n",
    "print(\"Output Data:\", output_df.shape)\n",
    "print(\"SOCA2016 Independent Data:\", soca2016_df[input_features].shape, soca2016_df[output_features].shape)\n",
    "print(\"ISPRS2016 Independent Data:\", isprs2020_df[input_features].shape, isprs2020_df[output_features].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Saving\n",
    "\n",
    "* We're going to save the data in the `global/interim/` path. This is to prevent any overwrites. \n",
    "* We also need to `index=True` for the savefile in order to preserve the metadata indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.to_csv(f\"{INTERIM_PATH.joinpath('inputs.csv')}\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Loading\n",
    "\n",
    "This is a tiny bit tricky if we want to preserve the meta data as the indices. So we need to set the index to be the same meta columns that we used last time via the `.set_index(meta_vars)` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs_df = pd.read_csv(f\"{INTERIM_PATH.joinpath('inputs.csv')}\")\n",
    "\n",
    "# add index\n",
    "test_inputs_df = test_inputs_df.set_index(meta_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##QUESTION(Ana): if we have alredy saved the file, couldn't we still use the input_df here? The one we saved it is supposed not to be modified , right? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Checking\n",
    "\n",
    "So curiously, we cannot compare the dataframes directly because there is some numerical error when saving them. But if we calculate the exact differences between them, we find that they are almost equal. See below what happens if we calculate the exact difference between the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not equal\n\nMismatched elements: 96056 / 642304 (15%)\nMax absolute difference: 1.42108547e-14\nMax relative difference: 3.92139227e-16\n x: array([[-4.704400e+00,  4.265410e+01,  2.546170e-02, ..., -3.458944e+00,\n        -1.017509e-02, -1.025450e+00],\n       [-9.015000e-01,  4.455050e+01,  2.060340e-02, ..., -3.691716e+00,...\n y: array([[-4.704400e+00,  4.265410e+01,  2.546170e-02, ..., -3.458944e+00,\n        -1.017509e-02, -1.025450e+00],\n       [-9.015000e-01,  4.455050e+01,  2.060340e-02, ..., -3.691716e+00,...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-41695177b4b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# example are they exactly the same?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# np.testing.assert_array_equal(test_inputs_df.describe(), input_df.describe())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_array_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_inputs_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/ml4ocn/lib/python3.6/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_array_equal\u001b[0;34m(x, y, err_msg, verbose)\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# Hide traceback for py.test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,\n\u001b[0;32m--> 936\u001b[0;31m                          verbose=verbose, header='Arrays are not equal')\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml4ocn/lib/python3.6/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[1;32m    844\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not equal\n\nMismatched elements: 96056 / 642304 (15%)\nMax absolute difference: 1.42108547e-14\nMax relative difference: 3.92139227e-16\n x: array([[-4.704400e+00,  4.265410e+01,  2.546170e-02, ..., -3.458944e+00,\n        -1.017509e-02, -1.025450e+00],\n       [-9.015000e-01,  4.455050e+01,  2.060340e-02, ..., -3.691716e+00,...\n y: array([[-4.704400e+00,  4.265410e+01,  2.546170e-02, ..., -3.458944e+00,\n        -1.017509e-02, -1.025450e+00],\n       [-9.015000e-01,  4.455050e+01,  2.060340e-02, ..., -3.691716e+00,..."
     ]
    }
   ],
   "source": [
    "# example are they exactly the same?\n",
    "# np.testing.assert_array_equal(test_inputs_df.describe(), input_df.describe())\n",
    "np.testing.assert_array_equal(test_inputs_df.values, input_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an assertion error that they're not equal. There is a mismatch difference of order 1e-15 for the absolute and relative differences. That's numerical error probably due to compression that comes when saving and loading data. Let's check again but with a little less expected precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_array_almost_equal(test_inputs_df.values, input_df.values, decimal=1e-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so just by reducing the precision by a smidge (1e-14 instead of 1e-15), we find that the arrays are the same. So we can trust it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###QUESTION(Ana):Should we save the data specifying that precision already?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.to_csv(f\"{INTERIM_PATH.joinpath('inputs_Ana.csv')}\", index=True, float_format='%.14f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 - Save the rest of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.to_csv(f\"{INTERIM_PATH.joinpath('inputs.csv')}\", index=True)\n",
    "output_df.to_csv(f\"{INTERIM_PATH.joinpath('outputs.csv')}\", index=True)\n",
    "soca2016_df.to_csv(f\"{INTERIM_PATH.joinpath('soca2016.csv')}\", index=True)\n",
    "isprs2020_df.to_csv(f\"{INTERIM_PATH.joinpath('isprs2020.csv')}\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-jax_py38]",
   "language": "python",
   "name": "conda-env-.conda-jax_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
