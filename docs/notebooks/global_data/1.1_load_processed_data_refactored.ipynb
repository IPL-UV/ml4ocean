{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo I.I - Loading the Data (Refactored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Important** - Paths and Directories\n",
    "\n",
    "This is annoying but it needs to be defined otherwise things get confusing. We need a few important paths to be pre-defined:\n",
    "\n",
    "| Name | Variable | Purpose |\n",
    "| ---| --- | --- |\n",
    "| Project | `PROJECT_PATH` | top level directory for the project (assuming megatron) |\n",
    "| Code |  `CODE_PATH` | folder of any dedicated functions that we use |\n",
    "| Raw Data | `RAW_PATH` | where the raw data is. Ideally, we **never** touch this ever except to read. |\n",
    "| Processed Data | `DATA_PATH` | where the processed data is stored |\n",
    "| Interim Data | `INTERIM_PATH` | where we save the training, validation and testing data |\n",
    "| Saved Models | `MODEL_PATH` | where we save any trained models |\n",
    "| Results Data | `RESULTS_PATH` | where we save any data results or outputs from ML models |\n",
    "| Figures | `FIG_PATH` | where we store any plotted figures during any part of our ML pipeline|\n",
    "\n",
    "This cell checks to see if all of the paths exist. If there is a path missing, it probably means you're not in megatron. If that's the case...well, we'll cross that bridge when we get there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "# define the top level directory\n",
    "PROJECT_PATH = pathlib.Path(\"/media/disk/erc/papers/2019_ML_OCN/\")\n",
    "CODE_PATH = PROJECT_PATH.joinpath(\"ml4ocean\")\n",
    "sys.path.append(str(CODE_PATH))\n",
    "\n",
    "# ml4ocean packages\n",
    "from src.utils import get_paths\n",
    "from src.data.world import get_full_data, world_features\n",
    "from src.features.world import subset_independent_floats\n",
    "\n",
    "PATHS = get_paths()\n",
    "\n",
    "# standard pacakges\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Global Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.world import get_input_data, get_meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wmo          int64\n",
       "n_cycle      int64\n",
       "N            int64\n",
       "lon        float64\n",
       "lat        float64\n",
       "juld       float64\n",
       "date        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = get_meta_data()\n",
    "t.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = get_input_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N               int64\n",
       "wmo             int64\n",
       "n_cycle         int64\n",
       "sla           float64\n",
       "PAR           float64\n",
       "RHO_WN_412    float64\n",
       "RHO_WN_443    float64\n",
       "RHO_WN_490    float64\n",
       "RHO_WN_555    float64\n",
       "RHO_WN_670    float64\n",
       "doy_sin       float64\n",
       "doy_cos       float64\n",
       "x_cart        float64\n",
       "y_cart        float64\n",
       "z_cart        float64\n",
       "PC1           float64\n",
       "PC2           float64\n",
       "PC3           float64\n",
       "PC4           float64\n",
       "PC5           float64\n",
       "PC6           float64\n",
       "PC7           float64\n",
       "PC1.1         float64\n",
       "PC2.1         float64\n",
       "PC3.1         float64\n",
       "PC1.2         float64\n",
       "PC2.2         float64\n",
       "PC3.2         float64\n",
       "PC4.1         float64\n",
       "bbp           float64\n",
       "bbp.1         float64\n",
       "bbp.2         float64\n",
       "bbp.3         float64\n",
       "bbp.4         float64\n",
       "bbp.5         float64\n",
       "bbp.6         float64\n",
       "bbp.7         float64\n",
       "bbp.8         float64\n",
       "bbp.9         float64\n",
       "bbp.10        float64\n",
       "bbp.11        float64\n",
       "bbp.12        float64\n",
       "bbp.13        float64\n",
       "bbp.14        float64\n",
       "bbp.15        float64\n",
       "bbp.16        float64\n",
       "bbp.17        float64\n",
       "bbp.18        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = get_full_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Training and Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Independent Set I (SOCA2016)\n",
    "\n",
    "This independent set has a set number of independent floats which are not counted in the training or validation phase. These floats were in a paper (Sauzede et. al., 2016) and used during the testing phase to showcase how well the models did.\n",
    "\n",
    "* 6901472\n",
    "* 6901493\n",
    "* 6901523\n",
    "* 6901496\n",
    "\n",
    "So we need to take these away from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, soca2016_df = subset_independent_floats(full_df, 'soca2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Indpendent Set II (ISPRS2020)\n",
    "\n",
    "This independent set was a set of floats taken from the ISPRS paper (Sauzede et. al., 2020 (pending...)). These floats were used as the independent testing set to showcase the performance of the ML methods.\n",
    "\n",
    "* 6901486 (North Atlantic?)\n",
    "* 3902121 (Subtropical Gyre?)\n",
    "\n",
    "So we need to take these away from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, isprs2020_df = subset_independent_floats(full_df, 'isprs2020')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - ML Data\n",
    "\n",
    "Now we want to subset the input data to be used for the ML models. Basically, we can subset all datasets that **are not** in the independent floats. In addition, we want all of the variables in the input features that we provided earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset non-independent flows\n",
    "dataset = 'both'\n",
    "ml_df, _ = subset_independent_floats(full_df, 'both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Inputs, Outputs\n",
    "\n",
    "Lastly, we need to split the data into training, validation (and possibly testing). Recall that all the inputs are already written above and the outputs as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = ml_df[world_features.input]\n",
    "output_df = ml_df[world_features.output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Final Dataset (saving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Print out data dimensions (w. metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data: (24704, 26)\n",
      "Output Data: (24704, 19)\n",
      "SOCA2016 Independent Data: (378, 26) (378, 19)\n",
      "ISPRS2016 Independent Data: (331, 26) (331, 19)\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Data:\", input_df.shape)\n",
    "print(\"Output Data:\", output_df.shape)\n",
    "print(\"SOCA2016 Independent Data:\", soca2016_df[world_features.input].shape, soca2016_df[world_features.output].shape)\n",
    "print(\"ISPRS2016 Independent Data:\", isprs2020_df[world_features.input].shape, isprs2020_df[world_features.output].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Saving\n",
    "\n",
    "* We're going to save the data in the `global/interim/` path. This is to prevent any overwrites. \n",
    "* We also need to `index=True` for the savefile in order to preserve the metadata indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so just by reducing the precision by a smidge (1e-14 instead of 1e-15), we find that the arrays are the same. So we can trust it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.to_csv(f\"{PATHS.data_interim.joinpath('inputs.csv')}\", index=True)\n",
    "output_df.to_csv(f\"{PATHS.data_interim.joinpath('outputs.csv')}\", index=True)\n",
    "soca2016_df.to_csv(f\"{PATHS.data_interim.joinpath('soca2016.csv')}\", index=True)\n",
    "isprs2020_df.to_csv(f\"{PATHS.data_interim.joinpath('isprs2020.csv')}\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-2019_egp]",
   "language": "python",
   "name": "conda-env-.conda-2019_egp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
