{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Task vs. Multi-Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be looking at the difference between multitask and multioutput. In a nutshell, multioutput is when we have a **single function** that is able to return the multidimensional vector of labels and multitask is when we have a **function for each output** of the multidimensional vector of labels. There are pros and cons to each of these methods so it's up to the expert to give the data scientists some reasons as to why one would choose one over the other.\n",
    "\n",
    "Let's define some terms: we have some dataset $\\mathcal{D}=\\{X,Y\\}$ of pairs of input $X \\in \\mathbb{R}^{N\\times D}$ and ouputs $Y \\in \\mathbb{R}^{N \\times O}$. Here $N$ is the number of samples, $D$ is the number of features/dimensions and $O$ are the number of outputs (or tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, ConstantKernel, RBF\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emmanuel/.conda/envs/sci_py36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Make Fake Dataset\n",
    "X, y = make_regression(\n",
    "    n_samples=1000, \n",
    "    n_features=10,    # Total Features\n",
    "    n_informative=3,   # Informative Features \n",
    "    n_targets=10,\n",
    "    bias=10,\n",
    "    noise=0.8,\n",
    "    random_state=123\n",
    "\n",
    ")\n",
    "\n",
    "# Training and Testing\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, train_size=500, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm I - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test I - MultiOutput Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.640\n",
      "MSE: 0.643\n",
      "RMSE: 0.802\n",
      "R2: 1.000 \n",
      "Time: 0.00388 seconds\n"
     ]
    }
   ],
   "source": [
    "linear_model = LinearRegression()\n",
    "t0 = time.time()\n",
    "linear_model.fit(xtrain, ytrain)\n",
    "t1 = time.time() - t0\n",
    "ypred = linear_model.predict(xtest)\n",
    "\n",
    "# Get Stats\n",
    "mae = mean_absolute_error(ypred, ytest)\n",
    "mse = mean_squared_error(ypred, ytest)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(ypred, ytest)\n",
    "\n",
    "print(\n",
    "    f\"MAE: {mae:.3f}\\nMSE: {mse:.3f}\\nRMSE: {rmse:.3f}\\nR2: {r2:.3f}\" \n",
    "    f\" \\nTime: {t1:.3} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test II - MultiTask Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.640\n",
      "MSE: 0.643\n",
      "RMSE: 0.802\n",
      "R2: 1.000 \n",
      "Time: 0.143 seconds\n"
     ]
    }
   ],
   "source": [
    "linear_model_multi = MultiOutputRegressor(\n",
    "    LinearRegression(), \n",
    "    n_jobs=-1,              # Number of cores to use to parallelize the training\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "linear_model_multi.fit(xtrain, ytrain)\n",
    "t1 = time.time() - t0\n",
    "ypred = linear_model_multi.predict(xtest)\n",
    "\n",
    "# Get Stats\n",
    "mae = mean_absolute_error(ypred, ytest)\n",
    "mse = mean_squared_error(ypred, ytest)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(ypred, ytest)\n",
    "\n",
    "print(\n",
    "    f\"MAE: {mae:.3f}\\nMSE: {mse:.3f}\\nRMSE: {rmse:.3f}\\nR2: {r2:.3f}\" \n",
    "    f\" \\nTime: {t1:.3} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are exactly the same in the case of Linear Regression. But for more complex models, we can expect that this will not be the same. One obvious trade-off is the number of tasks (outputs) that we have. 100 outputs is quite a lot so that requires a lot of time to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm II - Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GP algorithm does have multioutput so we can just pass in a vector of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP Model:\n",
      "MAE: 0.697\n",
      "MSE: 0.762\n",
      "RMSE: 0.873\n",
      "R2: 1.000 \n",
      "Time: 10.4 seconds\n"
     ]
    }
   ],
   "source": [
    "# define kernel function\n",
    "kernel = ConstantKernel() * RBF() + WhiteKernel()\n",
    "\n",
    "# define GP model\n",
    "gp_model = GaussianProcessRegressor(\n",
    "    kernel=kernel,            # kernel function (very important)\n",
    "    normalize_y=True,         # good standard practice\n",
    "    random_state=123,         # reproducibility\n",
    "    n_restarts_optimizer=10,  # good practice (avoids local minima)\n",
    ")\n",
    "\n",
    "# train GP Model\n",
    "t0 = time.time()\n",
    "gp_model.fit(xtrain, ytrain)\n",
    "t1 = time.time() - t0\n",
    "ypred = gp_model.predict(xtest)\n",
    "\n",
    "# Get Stats\n",
    "mae = mean_absolute_error(ypred, ytest)\n",
    "mse = mean_squared_error(ypred, ytest)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(ypred, ytest)\n",
    "\n",
    "print(\n",
    "    f\"GP Model:\\n\"\n",
    "    f\"MAE: {mae:.3f}\\nMSE: {mse:.3f}\\nRMSE: {rmse:.3f}\\nR2: {r2:.3f}\" \n",
    "    f\" \\nTime: {t1:.3} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP Model:\n",
      "MAE: 0.698\n",
      "MSE: 0.766\n",
      "RMSE: 0.875\n",
      "R2: 1.000 \n",
      "Time: 9.14 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define kernel function\n",
    "kernel = ConstantKernel() * RBF() + WhiteKernel()\n",
    "\n",
    "# Define GP Model\n",
    "gp_model = GaussianProcessRegressor(\n",
    "    kernel=kernel,            # kernel function (very important)\n",
    "    normalize_y=True,         # good standard practice\n",
    "    random_state=123,         # reproducibility\n",
    "    n_restarts_optimizer=10,  # good practice (avoids local minima)\n",
    ")\n",
    "\n",
    "# Define Multioutput function\n",
    "gp_model_multi = MultiOutputRegressor(\n",
    "    gp_model, \n",
    "    n_jobs=-1,              # Number of cores to use to parallelize the training\n",
    ")\n",
    "\n",
    "# Fit Model\n",
    "t0 = time.time()\n",
    "gp_model_multi.fit(xtrain, ytrain)\n",
    "t1 = time.time() - t0\n",
    "\n",
    "# Predict with test set\n",
    "ypred = gp_model_multi.predict(xtest)\n",
    "\n",
    "# Get Stats\n",
    "mae = mean_absolute_error(ypred, ytest)\n",
    "mse = mean_squared_error(ypred, ytest)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(ypred, ytest)\n",
    "\n",
    "print(\n",
    "    f\"GP Model:\\n\"\n",
    "    f\"MAE: {mae:.3f}\\nMSE: {mse:.3f}\\nRMSE: {rmse:.3f}\\nR2: {r2:.3f}\" \n",
    "    f\" \\nTime: {t1:.3} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Slightly** better accuracy but notice in this case the training time was similar. This sometimes happens because some algorithms have a difficult time converging when there are multiple outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm III - Bayesian Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm does not have multioutput functionality so instead we will use the multi-task functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP Model:\n",
      "MAE: 0.640\n",
      "MSE: 0.643\n",
      "RMSE: 0.802\n",
      "R2: 1.000 \n",
      "Time: 0.0338 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define GP Model\n",
    "bayes_model = BayesianRidge(\n",
    "    n_iter=1000,\n",
    "    normalize=True,         # good standard practice\n",
    "    verbose=1,              # Gives updates\n",
    ")\n",
    "\n",
    "# Define Multioutput function\n",
    "bayes_model_multi = MultiOutputRegressor(\n",
    "    bayes_model, \n",
    "    n_jobs=-1,              # Number of cores to use to parallelize the training\n",
    ")\n",
    "\n",
    "# Fit Model\n",
    "t0 = time.time()\n",
    "bayes_model_multi.fit(xtrain, ytrain)\n",
    "t1 = time.time() - t0\n",
    "\n",
    "# Predict with test set\n",
    "ypred = bayes_model_multi.predict(xtest)\n",
    "\n",
    "# Get Stats\n",
    "mae = mean_absolute_error(ypred, ytest)\n",
    "mse = mean_squared_error(ypred, ytest)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(ypred, ytest)\n",
    "\n",
    "print(\n",
    "    f\"GP Model:\\n\"\n",
    "    f\"MAE: {mae:.3f}\\nMSE: {mse:.3f}\\nRMSE: {rmse:.3f}\\nR2: {r2:.3f}\" \n",
    "    f\" \\nTime: {t1:.3} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sci_py36]",
   "language": "python",
   "name": "conda-env-sci_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
